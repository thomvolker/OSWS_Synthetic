---
title: "Practical 2: Evaluating utility and privacy of synthetic data"
subtitle: "Fake it ‘till you make it: Generating synthetic data with high utility in `R`"
author: "Thom Volker & Erik-Jan van Kesteren"
bibliography: osws.bib
link-citations: true
format: 
  html:
    toc: true
execute:
  error: true
---

---

__Note.__ This practical builds on [Practical 1](https://thomvolker.github.io/OSWS_Synthetic/practical/P1.html), and assumes you have completed all these exercises.

---


```{r}
#| label: create-synthetic-data
#| include: false

library(synthpop)  # to create synthetic data
library(ggplot2)   # required when using ggmice
library(patchwork) # to stitch multiple figures together
library(psych)     # to obtain descriptive statistics
library(purrr)     # to work with multiply imputed synthetic datasets
library(tidyverse) # to do some data wrangling

heart_failure <- readRDS(url("https://thomvolker.github.io/OSWS_Synthetic/data/heart_failure.RDS"))

syn_param <- syn(heart_failure, 
                 method = "parametric",
                 default.method = c("norm", "logreg", "polyreg", "polr"),
                 seed = 1)
syn_nonparam <- syn(heart_failure, method = "cart", seed = 1)
```


# Synthetic data utility

The quality of synthetic data sets can be assessed on multiple levels and in multiple different ways (e.g., quantitatively, but also visually). Starting on a univariate level, the distributions of the synthetic data sets can be compared with the distribution of the observed data. For categorical variables, the observed counts in each category can be compared between the real and synthetic data. For continuous variables, the density of the real and synthetic data can be compared. Later on, we also look at the utility of the synthetic data on a multivariate level.

---

## Univariate data utility

__1. To get an idea of whether creating the synthetic data went accordingly, compare the first 10 rows of the original data with the first 10 rows of the synthetic data sets (inspect both the parametric and the non-parametric set). Do you notice any differences?__

---

_Hint:_ You can extract the synthetic data from the synthetic data object by called `$syn` on the particular object.

::: {.callout-tip collapse=true}

## Show Code

```{r}
#| label: head-syn-data
#| eval: false
heart_failure |> head(10)
syn_param$syn |> head(10)
syn_nonparam$syn |> head(10)
```

:::

::: {.callout-warning icon=false collapse=true}

## Show Output

```{r}
#| label: head-syn-data-results
#| echo: false
heart_failure |>
  head(10) |>
  knitr::kable() |>
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) |>
  kableExtra::scroll_box(width = "100%")

syn_param$syn |> 
  head(10) |>
  knitr::kable() |>
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) |>
  kableExtra::scroll_box(width = "100%")

syn_nonparam$syn |>
  head(10) |>
  knitr::kable() |>
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) |>
  kableExtra::scroll_box(width = "100%")
```

You might notice that some of the continuous variables are not rounded as in the original data when using parametric synthesis models. Additionally, there are negative values in the synthetic version of the variable `creatinine_phosphokinase`, while the original data is strictly positive.

Both of these issues are not present when using CART, because CART draws values from the observed data (and thus can't create values that are not in the observed data).

:::

---

Apart from inspecting the data itself, we can assess distributional similarity between the observed and synthetic data.

---

__2. Compare the descriptive statistics from the synthetic data sets with the descriptive statistics from the observed data. What do you see?__

---

_Hint:_ Use the function `describe()` from the `psych` package to do this.

::: {.callout-tip collapse=true}

## Show Code

```{r}
#| label: compare-desc
#| eval: false
heart_failure |> 
  describe()

syn_param$syn |>
  describe()

syn_nonparam$syn |>
  describe()
```

:::

::: {.callout-warning icon=false collapse=true}

## Show Output

```{r}
#| label: compare-desc-results
#| echo: false

heart_failure |> 
  describe() |>
  round(2) |>
  knitr::kable(caption = "Observed data") |>
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) |>
  kableExtra::scroll_box(width = "100%")

syn_param$syn |>
  describe() |>
  round(2) |>
  knitr::kable(caption = "Synthetic data (parametric)") |>
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) |>
  kableExtra::scroll_box(width = "100%")

syn_nonparam$syn |>
  describe() |>
  round(2) |>
  knitr::kable(caption = "Synthetic data (non-parametric)") |>
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) |>
  kableExtra::scroll_box(width = "100%")
```


<br><br>

The descriptive statistics are not exactly similar, but come rather close in terms of mean and standard deviation. When looking at higher-order moments and the minimum and maximum, we see that there are some noticeable differences for parametrically synthesized data, but not so much for the non-parametrically synthesized data. We pay more attention to these issues when we visually inspect the synthetic data. 

:::

---

We will now visually compare the distributions of the observed and synthetic data, as this typically provides a more thorough understanding of the quality of the synthetic data.

---

__3. Use `compare()` from the synthpop package to compare the distributions of the observed and parametric synthetic data set, set the parameter `utility.stats = NULL`. What do you see?__

---

_For now, ignore the table below the figures, we will come to this at a later point._

::: {.callout-tip collapse=true}

## Show Code

```{r}
#| label: compare-parametric
#| eval: false

compare(syn_param, heart_failure)
```

:::

::: {.callout-warning icon=false collapse=true}

## Show Output

```{r}
#| label: compare-parametric-results
#| echo: false

compare.synds(syn_param, heart_failure)
```

You might notice that there are substantial differences between the distributions of some of the continuous variables. Especially for the variables `creatinine_phosphokinase`, `serum_creatinine` and `follow_up`, the synthetic data does not seem to capture the distribution of the observed data well. Also for the other variables, there are some discrepancies between the marginal distributions of the observed and synthetic data.

Of course, this could have been expected, since some of the variables are highly skewed, while we impose a normal distribution on each variable with the current set of parametric models. It is quite likely that we could have done a better job by using more elaborate data manipulation (e.g., transforming variables such that there distribution corresponds more closely to a normal distribution (and back-transforming afterwards)).

For the categorical variables, we seem to be doing a decent job on the marginal levels, as there are only small differences between the observed and synthetic frequencies in each level.

:::

---

__4. Use `compare()` from the synthpop package to compare the distributions of the observed and non-parametric synthetic data set, set the parameter `utility.stats = NULL`. What do you see?__

---

_Again, ignore the table below the figures, we will come to this at a later point._

::: {.callout-tip collapse=true}

## Show Code

```{r}
#| label: compare-nonparametric
#| eval: false

compare(syn_nonparam, heart_failure)
```

:::

::: {.callout-warning icon=false collapse=true}

## Show Output

```{r}
#| label: compare-nonparametric-results
#| echo: false

compare.synds(syn_nonparam, heart_failure)
```

Using non-parametric synthesis models (i.e., CART), we do a much better job in recreating the shape of the original data. In fact, the marginal distributions are close to identical, including all irregularities in the original data.

:::


---

There are also other, more formal, ways to assess the utility of the synthetic data, although there is some critique against these methods [see, e.g., @drechsler_utility_psd]. Here, we will discuss one of these measures, the $pMSE$, but there are others (although utility measures tend to correlate strongly in general). The intuition behind the $pMSE$ is to predict whether an observation is actually observed, or a synthetic record. If this is possible, the observed and synthetic data differ on at least one dimension, which allows to distinguish between the records.

Formally, the $pMSE$ is defined as 
$$
pMSE = \frac{1}{n_{obs} + n_{syn}}
\Bigg(
\sum^{n_{obs}}_{i=1} \Big(\hat{\pi}_i - \frac{n_{obs}}{n_{obs} + n_{syn}}\Big)^2 + 
\sum^{n_{obs} + n_{syn}}_{i={(n_{obs} + 1)}} \Big(\hat{\pi_i} - \frac{n_{syn}}{n_{obs} + n_{syn}}\Big)^2
\Bigg),
$$
which, in our case, simplifies to 
$$
pMSE = \frac{1}{`r 2 * nrow(heart_failure)`}
\Bigg(
\sum^{n_{obs} + n_{syn}}_{i=1} \Big(\hat{\pi}_i - 0.5\Big)^2
\Bigg),
$$
where $n_{obs}$ and $n_{syn}$ are the sample sizes of the observed and synthetic data, $\hat{\pi}_i$ is the probability of belonging to the synthetic data.

---

__5. Calculate the $pMSE$ for the variable `creatinine_phosphokinase` for both synthetic sets and compare the values between both synthesis methods. Use a logistic regression model to create the probabilities $\pi$. What do you see?__

---

_Hint:_ You can use the function `utility.gen()` and set the arguments `method = "logit"` (this denotes the model used to predict the probabilities), `vars = "creatinine_phosphokinase"` and `maxorder = 0` (which denotes that we don't want to specify interactions, as we only have a single variable here).

::: {.callout-tip collapse=true}

## Show Code

```{r}
#| label: pmse-creatinine-calc
#| eval: false

utility.gen(syn_param, 
            heart_failure, 
            method = "logit", 
            vars = "creatinine_phosphokinase",
            maxorder = 0)

utility.gen(syn_nonparam, 
            heart_failure, 
            method = "logit", 
            vars = "creatinine_phosphokinase",
            maxorder = 0)

```

:::

::: {.callout-warning icon=false collapse=true}

## Show Output

```{r}
#| label: pmse-creatinine-results
#| echo: false

utility.gen(syn_param, 
            heart_failure, 
            method = "logit", 
            vars = "creatinine_phosphokinase",
            maxorder = 0)

utility.gen(syn_nonparam, 
            heart_failure, 
            method = "logit", 
            vars = "creatinine_phosphokinase",
            maxorder = 0)
```

The $pMSE$ is about seven times higher for the parametrically synthesized data set.

It can be hard to interpret the values of the $pMSE$, because they say little about how useful the synthetic data is in general. To get a more insightful measure, we can take ratio of the calculated $pMSE$ over the expected $pMSE$ under the null distribution of a *correct* synthesis model (i.e., in line with the data-generating model). The $pMSE$ ratio is given by
$$
\begin{aligned}
pMSE \text{ ratio } &= 
\frac{pMSE}
{(k-1)(\frac{n_{\text{obs}}}{n_{\text{syn}} + n_{\text{obs}}})^2(\frac{n_{\text{syn}}}{n_{\text{syn}} + n_{\text{obs}}}) / (n_{\text{obs}} + n_{\text{syn}})} \\ &=
\frac{pMSE}{(k-1)(\frac{1}{2})^3/(n_{obs} + n_{syn})},
\end{aligned}
$$
where $k$ denotes the number of predictors in the propensity score model, including the intercept. Note that this formulation only holds for a $pMSE$ that is obtained through logistic regression. When different methods are used to calculate the probabilities, the null distribution can be obtained by using a permutation test.

Ideally, the $pMSE$ ratio equals $1$, but according to the `synthpop` authors, values below $3$ are indicative of high quality synthetic data, while values below $10$ are deemed acceptable [@raab2021utility]. This would indicate that both synthesis models are very good models to synthesize the variable `creatinine_phosphokinase`. However, our logistic regression model only evaluates whether the mean of the two variables is similar, and might thus not be the best model for evaluating the quality of the synthetic data in this case.

:::

---

__6. Recalculate the $pMSE$ for the variable `creatinine_phosphokinase` for both synthetic sets, but this time using a CART model to estimate the probabilities $\pi$. What do you see?__


---

_Hint:_ You can again use the function `utility.gen()` and set the arguments `method = "cart"` and `vars = "creatinine_phosphokinase"`.

::: {.callout-tip collapse=true}

## Show Code

```{r}
#| label: pmse-cart-creatinine-calc
#| eval: false

utility.gen(syn_param, 
            heart_failure, 
            method = "cart", 
            vars = "creatinine_phosphokinase")

utility.gen(syn_nonparam, 
            heart_failure, 
            method = "cart", 
            vars = "creatinine_phosphokinase")

```

:::

::: {.callout-warning icon=false collapse=true}

## Show Output

```{r}
#| label: pmse-cart-creatinine-results
#| echo: false

utility.gen(syn_param, 
            heart_failure, 
            method = "cart", 
            vars = "creatinine_phosphokinase")

utility.gen(syn_nonparam, 
            heart_failure, 
            method = "cart", 
            vars = "creatinine_phosphokinase")
```

The $pMSE$-ratio is about four times higher for the parametrically synthesized data set using the CART model to estimate the probabilities $\pi$. This indicates that the nonparametric synthesis method is better at reproducing the variable `creatinine_phosphokinase`. However, both $pMSE$-ratio values are still well below $10$, indicating reasonable synthetic data quality, where I would argue that the parametric synthetic version of `creatinine_phosphokinase` is a poor representation of the original data.

:::

---

## Multivariate data utility

Being able to reproduce the original univariate distributions is a good first step, but generally the goal of synthetic data reaches beyond that. Specifically, we often want to reproduce the relationships between the variables in the data.
In the previous section, we saw that an evaluation of utility is often best carried out through visualizations. However, creating visualizations is cumbersome for multivariate relationships.
Creating visualizations beyond bivariate relationships is often not feasible, whereas displaying all bivariate relationships in the data already results in $p(p-1)/2$ different figures. 


In the synthetic data literature, a distinction is often made between general and specific utility measures. General utility measures assess to what extent the relationships between combinations of variables (and potential interactions between them) are preserved in the synthetic data set. These measures are often for pairs of variables, or for all combinations of variables. Specific utility measures focus, as the name already suggests, on a specific analysis. This analysis is performed on the observed data and the synthetic data, and the similarity between inferences on these data sets is quantified.

---

### General utility measures

---

Continuing with our $pMSE$ approach, we can inspect which variables can predict whether observations are “true” or “synthetic” using the $pMSE$-ratio, similarly to what we just did using individual variables. We first try to predict the class of all observations by using all variables simultaneously, and hereafter we look at the results for all unique pairs of variables in the data.


---

__7. Use the function `utility.gen()` from the `synthpop` package to calculate the $pMSE$-ratio using all variables for both synthetic sets. What do you see?__

---

::: {.callout-tip collapse=true}

## Show Code

```{r}
#| label: utility-all
#| eval: false
utility.gen(syn_param, heart_failure)
utility.gen(syn_nonparam, heart_failure)
```
:::

::: {.callout-warning icon=false collapse=true}

## Show Output

```{r}
#| label: utility-all-results
#| echo: false
#| 
utility.gen(syn_param, heart_failure, print.flag = F)
utility.gen(syn_nonparam, heart_failure, print.flag = F)
```

The `CART` model was somewhat better, but the difference is relatively small. To get more insight into which variables and bivariate relationships were synthesized accordingly, and which can be improved, we can use `utility.tables.list()`.

:::

---

__8. Use the function `utility.tables()` from the `synthpop` package to calculate the $pMSE$-ratio for each pair of variables for both synthetic sets. What do you see?__

---

_Hint:_ To use the same color scale for both synthetic data sets, you can set the arguments `min.scale = 0` and `max.scale = 45`.

::: {.callout-tip collapse=true}

## Show Code

```{r}
#| label: pairwise-utility
#| eval: false
utility.tables(syn_param, heart_failure, min.scale = 0, max.scale = 45)
utility.tables(syn_nonparam, heart_failure, min.scale = 0, max.scale = 45)
```

:::

::: {.callout-warning icon=false collapse=true}

## Show Output

```{r}
#| label: pairwise-utility-results
#| echo: false
utility.tables(syn_param, heart_failure, min.scale = 0, max.scale = 45)
utility.tables(syn_nonparam, heart_failure, min.scale = 0, max.scale = 45)
```


Here, we finally see that our parametric synthesis model is severely flawed. Quite some of the $pMSE$ ratios are larger than 20, which means that we did in poor job in synthesizing these variables or the relationship of these variables with other variables. Note that we partly knew this already from our visualizations. Our non-parametric synthesis model is doing very good. The highest $pMSE$-ratio values are (much) smaller than $10$, which actually indicates that our synthetic data are of high quality.


:::

---

### Specific utility measures

Specific utility measures assess whether the same analysis on the observed and the synthetic data gives similar results. Say that we are interested in, for instance, the relationship between whether a person survives, the age of this person, whether this person has diabetes and whether or not this person smokes, including the follow-up time as a control variable in the model.

---

__9. Fit this model as a logistic regression model using `glm.synds()` with `family = binomial` and `data = synthetic_data_object`. Compare the results obtained with both synthetic data sets with the results obtained on the original data. What do you see?__

_Hint:_ You can also use `compare.fit.synds()` to compare the results of the models fitted on the synthetic data sets with the model fitted on the observed data.

---

::: {.callout-tip collapse=true}

## Show Code

```{r}
#| label: specific-utility

fit_param <- glm.synds(deceased ~ age + diabetes + smoking + follow_up,
                       family = binomial, 
                       data = syn_param)

fit_nonparam <- glm.synds(deceased ~ age + diabetes + smoking + follow_up,
                          family = binomial, 
                          data = syn_nonparam)

fit_obs <- glm(deceased ~ age + diabetes + smoking + follow_up,
               family = binomial, 
               data = heart_failure)
```

:::

::: {.callout-warning icon=false collapse=true}

## Show Output

```{r}
#| label: specific-utility-results
summary(fit_param)
summary(fit_nonparam)
summary(fit_obs)
```

```{r}
#| label: specific-utility-compare

compare.fit.synds(fit_param, heart_failure)
compare.fit.synds(fit_nonparam, heart_failure)
```

The results obtained for both synthetic data sets are quite similar, but the parametrically synthesized data are somewhat closer to the results from the analysis on the real data than the non-parametrically synthesized data.
This is quite paradoxical, as we saw before that the non-parametric synthesis model yielded much more realistic data than the parametric synthesis model.
This shows an important mismatch between general and specific utility. That is, to obtain high specific utility, it is not necessary to have high general utility. Moreover, high general utility does not guarantee high specific utility. 
Additionally, these results show that synthetic data with lower general utility can still be very useful if the goal is to perform specific analyses.

:::


---

# Statistical disclosure control

---


Synthetic data can provide a relatively safe framework for sharing data. However, some risks will remain present, and it is important to evaluate these risks. For example, it can be the case that the synthesis models were so complex that the synthetic records are very similar or even identical to the original records, which can lead to privacy breaches. 

::: {.callout-important collapse=false}

## Privacy of synthetic data

Synthetic data by itself does not provide any formal privacy guarantees. These guarantees can be incorporated, for example by using differentially private synthesis methods. However, these methods are not yet widely available in `R`. If privacy is not built-in by design, it remains important to inspect the synthetic data for potential risks. Especially if you're not entirely sure, it is better to stay at the safe side: use relatively simple, parametric models, check for outliers, and potentially add additional noise to the synthetic data. See also Chapter 4 in the book [_Synthetic Data for Official Statistics_](https://unece.org/sites/default/files/2022-11/ECECESSTAT20226.pdf).

:::

---

__10. Call the function `replicated.uniques()` on the synthetic data. This function checks whether there are duplicates of observations that were unique in the original data.__

---

::: {.callout-tip collapse=true}

## Show Code

```{r}
#| label: check-duplicated
#| eval: false

replicated.uniques(syn_param, heart_failure)
replicated.uniques(syn_nonparam, heart_failure)
```
:::

::: {.callout-warning icon=false collapse=true}

## Show Output

```{r}
#| label: check-duplicated-results
#| echo: false

replicated.uniques(syn_param, heart_failure)
replicated.uniques(syn_nonparam, heart_failure)
```

None of observations occur repeatedly, so we have not accidentally copied any of the “true” observations into the synthetic sets. This provides some safeguard against accidentally releasing sensitive information. However, if the data contains really sensitive information, this might not be enough, and one could for example check whether the synthetic data differs from the observed data along multiple dimensions (i.e., variables). Such additional checks depend on the problem at hand. Additionally, one might want to take additional measures against accidentally disclosing information about observations, for example by drawing some of the variables from a parametric distribution. Even before distribution synthetic data, think wisely about whether there may remain any disclosure risks with respect to the data that will be distributed.

:::

--- 

If you find the synthetic data too risky to be released as is, you can impose additional statistical disclosure limitation techniques, that additionally reduce the information in the synthetic data. For example, you can add noise to the synthetic data by using smoothing, or you can impose additional top/bottom coding, such that extreme values cannot appear in the synthetic data. This is easily done using the function `sdc()` as implemented in `synthpop`. Which statistical disclosure techniques to apply typically depends on the problem at hand, the sensitivity of the data and the synthesis strategies used. For example, for our non-parametric synthesis strategy, we re-use observed values, which might lead to an unacceptable risk of disclosure. Then, we could apply smoothing to the synthetic data to reduce the risk of disclosure. 


# Inferences from synthetic data

Lastly, when you have obtained a synthetic data set and want to make inferences from this set, you have to be careful, because generating synthetic data adds variance to the already present sampling variance that you take into account when evaluating hypotheses. 
Specifically, if you want to make inferences with respect to the sample of original observations, you can use unaltered analysis techniques and corresponding, conventional standard errors. 

However, if you want to inferences with respect to the population the sample is taken from, you will have to adjust the standard errors, to account for the fact that the synthesis procedure adds additional variance. 
The amount of variance that is added, depends on the number of synthetic data sets that are generated.
Intuitively, when generating multiple synthetic data sets, the additional random noise that is induced by the synthesis cancels out, making the parameter estimates more stable. 

There are two ways to obtain statistically valid results from synthetic data. The first requires that you have multiple synthetic data sets, and estimates the variance between the obtained estimates in each of the synthetic data sets. The corresponding pooling rules are presented in @reiter_partially_inference_2003. For scalar $Q$, with $q^{(i)}$ and $u^{(i)}$ the point estimate and the corresponding variance estimate in synthetic data set $D^{(i)}$ for $i = 1, \dots, m$, the following quantities are needed for inferences:

$$
\begin{aligned}
\bar{q}_m &= \sum_{i=1}^m \frac{q^{(i)}}{m}, \\
b_m &= \sum_{i=1}^m \frac{(q^{(i)} - \bar{q}_m)}{m-1}, \\
\bar{u}_m &= \sum_{i=1}^m \frac{u^{(i)}}{m}.
\end{aligned}
$$

The analyst can use $\bar{q}_m$ to estimate $Q$ and 
$$
T_p = \frac{b_m}{m} + \bar{u}_m
$$
to estimate the variance of $\bar{q}_m$. Then, $\frac{b_m}{m}$ is the correction factor for the additional variance due to using a finite number of imputations. 

The second way to obtain statistically valid results from synthetic data allows for multiple synthetic data sets, but does not require it [@raab2016practical]. In this case, the between-imputation variance is estimated from the standard error(s) of the estimates, which simplifies the total variance of each estimate to
$$
T_s = \frac{\bar{u}_m}{m} + \bar{u}_m.
$$
When you have $m = 1$ synthetic data set, we have $T_s = 2u$, where $u$ is the variance estimate obtained in that synthetic set.

